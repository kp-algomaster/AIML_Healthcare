{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font color='blue'>Table of contents</font>\n",
                "\n",
                "- [LeNet5 Architecture](#lenet)\n",
                "- [Display the Network](#display)\n",
                "- [Get the Fashion-MNIST Data](#get-data)\n",
                "- [System Configuration](#sys-config)\n",
                "- [Training Configuration](#train-config)\n",
                "- [System Setup](#sys-setup)\n",
                "- [Training](#training)\n",
                "- [Validation](#validation)\n",
                "- [Main function](#main)\n",
                "- [Plot Loss](#plot-loss)\n",
                "- [Miscellaneous](#misc)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:blue\">Convolutional Neural Network Using Batch Normalization</font>\n",
                "\n",
                "In this notebook, we  add batch norm layers to the LeNet network, and see how it affects network training and convergence.\n",
                "\n",
                "Instead of the MNIST dataset, which overfits easily, we will use the Fashion MNIST dataset.\n",
                "\n",
                "The figure below shows some samples from the Fashion MNIST dataset.\n",
                "\n",
                "<img src=\"https://www.learnopencv.com/wp-content/uploads/2021/01/c3-w3-fashion-mnist-sprite.jpg\" width=\"600\">\n",
                "\n",
                "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
                "\n",
                "| Label | Description |\n",
                "| --- | --- |\n",
                "| 0 | T-shirt/top |\n",
                "| 1 | Trouser |\n",
                "| 2 | Pullover |\n",
                "| 3 | Dress |\n",
                "| 4 | Coat |\n",
                "| 5 | Sandal |\n",
                "| 6 | Shirt |\n",
                "| 7 | Sneaker |\n",
                "| 8 | Bag |\n",
                "| 9 | Ankle boot |\n",
                "\n",
                "\n",
                "\n",
                "We want to classify images in this dataset, using the LeNet network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt  # one of the best graphics library for python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "\n",
                "from typing import Iterable\n",
                "from dataclasses import dataclass\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "from torchvision import datasets, transforms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">1. LeNet Architecture with BatchNorm</font><a name=\"lenet\"></a>\n",
                "\n",
                "We have already explained the architecture for LeNet in the previous notebook.\n",
                "\n",
                "Here, we create another model called LeNetBN, adding Batch Normalization layers to the 2 convolution blocks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "class LeNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "        # convolution layers\n",
                "        self._body = nn.Sequential(\n",
                "            # First convolution Layer\n",
                "            # input size = (32, 32), output size = (28, 28)\n",
                "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            # Max pool 2-d\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            \n",
                "            # Second convolution layer\n",
                "            # input size = (14, 14), output size = (10, 10)\n",
                "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            # output size = (5, 5)\n",
                "        )\n",
                "        \n",
                "        # Fully connected layers\n",
                "        self._head = nn.Sequential(\n",
                "            # First fully connected layer\n",
                "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
                "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # second fully connected layer\n",
                "            # in_features = output of last linear layer = 120 \n",
                "            nn.Linear(in_features=120, out_features=84), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # Third fully connected layer which is also output layer\n",
                "            # in_features = output of last linear layer = 84\n",
                "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
                "            nn.Linear(in_features=84, out_features=10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        # apply feature extractor\n",
                "        x = self._body(x)\n",
                "        # flatten the output of conv layers\n",
                "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
                "        x = x.view(x.size()[0], -1)\n",
                "        # apply classification head\n",
                "        x = self._head(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "class LeNetBN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "        # convolution layers\n",
                "        self._body = nn.Sequential(\n",
                "            # First convolution Layer\n",
                "            # input size = (32, 32), output size = (28, 28)\n",
                "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
                "            nn.BatchNorm2d(6),\n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            # Max pool 2-d\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            \n",
                "            # Second convolution layer\n",
                "            # input size = (14, 14), output size = (10, 10)\n",
                "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
                "            nn.BatchNorm2d(16),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            # output size = (5, 5)\n",
                "        )\n",
                "        \n",
                "        # Fully connected layers\n",
                "        self._head = nn.Sequential(\n",
                "            # First fully connected layer\n",
                "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
                "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # second fully connected layer\n",
                "            # in_features = output of last linear layer = 120 \n",
                "            nn.Linear(in_features=120, out_features=84), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # Third fully connected layer. It is also output layer\n",
                "            # in_features = output of last linear layer = 84\n",
                "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
                "            nn.Linear(in_features=84, out_features=10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        # apply feature extractor\n",
                "        x = self._body(x)\n",
                "        # flatten the output of conv layers\n",
                "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
                "        x = x.view(x.size()[0], -1)\n",
                "        # apply classification head\n",
                "        x = self._head(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">2. Display the Network</font><a name=\"display\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "lenet_model = LeNet()\n",
                "print(lenet_model)\n",
                "lenetBN_model = LeNetBN()\n",
                "print(lenetBN_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "lines_to_next_cell": 2
            },
            "source": [
                "## <font style=\"color:green\">3. Get Fashion-MNIST Data</font><a name=\"get-data\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def get_data(batch_size, data_root='data', num_workers=1):\n",
                "    \n",
                "    train_test_transforms = transforms.Compose([\n",
                "        # Resize to 32X32\n",
                "        transforms.Resize((32, 32)),\n",
                "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
                "        transforms.ToTensor(),\n",
                "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
                "        # This mean and variance is calculated on training data (verify for yourself)\n",
                "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
                "    ])\n",
                "    \n",
                "    # train dataloader\n",
                "    train_loader = torch.utils.data.DataLoader(\n",
                "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=True,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    \n",
                "    # test dataloader\n",
                "    test_loader = torch.utils.data.DataLoader(\n",
                "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    return train_loader, test_loader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">4. System Configuration</font><a name=\"sys-config\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class SystemConfiguration:\n",
                "    '''\n",
                "    Describes the common system setting needed for reproducible training\n",
                "    '''\n",
                "    seed: int = 42  # seed number to set the state of all random number generators\n",
                "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
                "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">5. Training Configuration</font><a name=\"train-config\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfiguration:\n",
                "    '''\n",
                "    Describes configuration of the training process\n",
                "    '''\n",
                "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
                "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
                "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
                "    log_interval: int = 100  # how many batches to wait between logging training status\n",
                "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
                "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
                "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
                "    device: str = 'cuda'  # device to use for training.\n",
                "    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">6. System Setup</font><a name=\"sys-setup\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def setup_system(system_config: SystemConfiguration) -> None:\n",
                "    torch.manual_seed(system_config.seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
                "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">7. Training</font><a name=\"training\"></a>\n",
                "We are familiar with the training pipeline used in PyTorch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def train(\n",
                "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
                "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
                ") -> None:\n",
                "    \n",
                "    # change model in training mode\n",
                "    model.train()\n",
                "    \n",
                "    # to get batch loss\n",
                "    batch_loss = np.array([])\n",
                "    \n",
                "    # to get batch accuracy\n",
                "    batch_acc = np.array([])\n",
                "        \n",
                "    for batch_idx, (data, target) in enumerate(train_loader):\n",
                "        \n",
                "        # clone target\n",
                "        indx_target = target.clone()\n",
                "        # send data to device (its is medatory if GPU has to be used)\n",
                "        data = data.to(train_config.device)\n",
                "        # send target to device\n",
                "        target = target.to(train_config.device)\n",
                "\n",
                "        # reset parameters gradient to zero\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # forward pass to the model\n",
                "        output = model(data)\n",
                "        \n",
                "        # cross entropy loss\n",
                "        loss = F.cross_entropy(output, target)\n",
                "        \n",
                "        # find gradients w.r.t training parameters\n",
                "        loss.backward()\n",
                "        # Update parameters using gradients\n",
                "        optimizer.step()\n",
                "        \n",
                "        batch_loss = np.append(batch_loss, [loss.item()])\n",
                "        \n",
                "        # get probability score using softmax\n",
                "        prob = F.softmax(output, dim=1)\n",
                "            \n",
                "        # get the index of the max probability\n",
                "        pred = prob.data.max(dim=1)[1]  \n",
                "                        \n",
                "        # correct prediction\n",
                "        correct = pred.cpu().eq(indx_target).sum()\n",
                "            \n",
                "        # accuracy\n",
                "        acc = float(correct) / float(len(data))\n",
                "        \n",
                "        batch_acc = np.append(batch_acc, [acc])\n",
                "\n",
                "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
                "            print(\n",
                "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
                "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
                "                )\n",
                "            )\n",
                "            \n",
                "    epoch_loss = batch_loss.mean()\n",
                "    epoch_acc = batch_acc.mean()\n",
                "    return epoch_loss, epoch_acc"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">8. Validation</font><a name=\"validation\"></a>\n",
                "\n",
                "After every few epochs **`validation`** is called, with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
                "\n",
                "**Note:** We use `model.eval()` to enable evaluation mode of the model. This will stop calculating the running estimate of mean and variance of data. Using instead just the mean and variance computed while training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def validate(\n",
                "    train_config: TrainingConfiguration,\n",
                "    model: nn.Module,\n",
                "    test_loader: torch.utils.data.DataLoader,\n",
                ") -> float:\n",
                "    # \n",
                "    model.eval()\n",
                "    test_loss = 0\n",
                "    count_corect_predictions = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for data, target in test_loader:\n",
                "            indx_target = target.clone()\n",
                "            data = data.to(train_config.device)\n",
                "\n",
                "            target = target.to(train_config.device)\n",
                "\n",
                "            output = model(data)\n",
                "            # add loss for each mini batch\n",
                "            test_loss += F.cross_entropy(output, target).item()\n",
                "\n",
                "            # get probability score using softmax\n",
                "            prob = F.softmax(output, dim=1)\n",
                "\n",
                "            # get the index of the max probability\n",
                "            pred = prob.data.max(dim=1)[1] \n",
                "\n",
                "            # add correct prediction count\n",
                "            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
                "\n",
                "        # average over number of mini-batches\n",
                "        test_loss = test_loss / len(test_loader)  \n",
                "\n",
                "        # average over number of dataset\n",
                "        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
                "\n",
                "        print(\n",
                "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
                "                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
                "            )\n",
                "        )\n",
                "    return test_loss, accuracy/100.0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">9. Main</font><a name=\"main\"></a>\n",
                "\n",
                "\n",
                "Here, we use the configuration parameters defined above and start  training. \n",
                "\n",
                "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
                "1. Load the data using dataloaders.\n",
                "1. Create an instance of the LeNet model.\n",
                "1. Specify optimizer to use.\n",
                "1. Set up variables to track loss and accuracy and start training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
                "    \n",
                "    # system configuration\n",
                "    setup_system(system_configuration)\n",
                "\n",
                "    # batch size\n",
                "    batch_size_to_set = training_configuration.batch_size\n",
                "    # num_workers\n",
                "    num_workers_to_set = training_configuration.num_workers\n",
                "    # epochs\n",
                "    epoch_num_to_set = training_configuration.epochs_count\n",
                "\n",
                "    # if GPU is available use training config, \n",
                "    # else lower batch_size, num_workers and epochs count\n",
                "    if torch.cuda.is_available():\n",
                "        device = \"cuda\"\n",
                "    elif torch.backends.mps.is_available():\n",
                "        device = torch.device(\"mps\")\n",
                "    else:\n",
                "        device = \"cpu\"\n",
                "        batch_size_to_set = 16\n",
                "        num_workers_to_set = 2\n",
                "        epoch_num_to_set = 10\n",
                "\n",
                "    # data loader\n",
                "    train_loader, test_loader = get_data(\n",
                "        batch_size=batch_size_to_set,\n",
                "        data_root=training_configuration.data_root,\n",
                "        num_workers=num_workers_to_set\n",
                "    )\n",
                "    \n",
                "    # Update training configuration\n",
                "    training_configuration = TrainingConfiguration(\n",
                "        device=device,\n",
                "        epochs_count=epoch_num_to_set,\n",
                "        batch_size=batch_size_to_set,\n",
                "        num_workers=num_workers_to_set\n",
                "    )\n",
                "        \n",
                "    # send model to device (GPU/CPU)\n",
                "    model.to(training_configuration.device)\n",
                "\n",
                "    # optimizer\n",
                "    optimizer = optim.SGD(\n",
                "        model.parameters(),\n",
                "        lr=training_configuration.learning_rate\n",
                "    )\n",
                "\n",
                "    best_loss = torch.tensor(np.inf)\n",
                "    \n",
                "    # epoch train/test loss\n",
                "    epoch_train_loss = np.array([])\n",
                "    epoch_test_loss = np.array([])\n",
                "    \n",
                "    # epch train/test accuracy\n",
                "    epoch_train_acc = np.array([])\n",
                "    epoch_test_acc = np.array([])\n",
                "    \n",
                "    # trainig time measurement\n",
                "    t_begin = time.time()\n",
                "    for epoch in range(training_configuration.epochs_count):\n",
                "        \n",
                "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
                "        \n",
                "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
                "        \n",
                "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
                "\n",
                "        elapsed_time = time.time() - t_begin\n",
                "        speed_epoch = elapsed_time / (epoch + 1)\n",
                "        speed_batch = speed_epoch / len(train_loader)\n",
                "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
                "        \n",
                "        print(\n",
                "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
                "                elapsed_time, speed_epoch, speed_batch, eta\n",
                "            )\n",
                "        )\n",
                "\n",
                "        if epoch % training_configuration.test_interval == 0:\n",
                "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
                "            \n",
                "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
                "        \n",
                "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
                "            \n",
                "            if current_loss < best_loss:\n",
                "                best_loss = current_loss\n",
                "                \n",
                "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
                "    \n",
                "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "model = LeNet()\n",
                "modelBN = LeNetBN() \n",
                "\n",
                "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
                "\n",
                "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">10. Plot Loss</font> <a name=\"plot-loss\"></a>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "lines_to_next_cell": 2
            },
            "outputs": [],
            "source": [
                "# Plot loss\n",
                "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
                "x = range(len(epoch_train_loss))\n",
                "\n",
                "\n",
                "plt.figure\n",
                "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
                "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
                "\n",
                "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
                "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
                "\n",
                "plt.xlabel('epoch no.')\n",
                "plt.ylabel('loss')\n",
                "plt.legend(loc='upper right')\n",
                "plt.title('Training and Validation Loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The above curves show that when we use Batch Normalization, the training converges faster."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:blue\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font> <a name=\"misc\"></a>\n",
                "\n",
                "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST. Refrain, even when you find many continuing to do this, simply because it does not have a profound effect on the results.\n",
                "\n",
                "Let us find  the mean and standard deviation for Fashion MNIST and use it instead of MNIST.\n",
                "\n",
                "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and then use the functions given below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torchvision\n",
                "train_transform = transforms.Compose([transforms.ToTensor()])\n",
                "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
                "\n",
                "print(train_set.data.float().mean()/255)\n",
                "print(train_set.data.float().std()/255)"
            ]
        }
    ],
    "metadata": {
        "jupytext": {
            "encoding": "# -*- coding: utf-8 -*-"
        },
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python38"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
